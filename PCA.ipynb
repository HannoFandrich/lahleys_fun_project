{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  brand     id   mois   prot    fat   ash  sodium  carb   cal\n",
      "0     A  14069  27.82  21.43  44.87  5.11    1.77  0.77  4.93\n",
      "1     A  14053  28.49  21.26  43.89  5.34    1.79  1.02  4.84\n",
      "2     A  14025  28.35  19.99  45.78  5.08    1.63  0.80  4.95\n",
      "3     A  14016  30.55  20.15  43.13  4.79    1.61  1.38  4.74\n",
      "4     A  14005  30.49  21.28  41.65  4.82    1.64  1.76  4.67\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('pizza.csv')\n",
    "print(data.head())\n",
    "\n",
    "data=data.drop(['brand','id'], axis=1)\n",
    "#data=data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.47093729e-02  6.28275866e-01  4.21668945e-01 -2.20721584e-01\n",
      "   6.47029345e-03 -4.46449902e-01 -4.18569035e-01]\n",
      " [-3.78760904e-01  2.69706650e-01 -7.46027442e-01 -1.05931982e-02\n",
      "   3.87982788e-01  1.71520299e-04 -2.76764643e-01]\n",
      " [-4.46665915e-01 -2.34379085e-01  1.99308714e-01 -5.07042158e-01\n",
      "  -1.73367634e-01  5.25402868e-01 -3.77671525e-01]\n",
      " [-4.71889526e-01  1.10990417e-01 -5.62726863e-02  5.52398549e-01\n",
      "  -6.70885701e-01 -5.88609281e-02 -5.60214003e-02]\n",
      " [-4.35702887e-01 -2.01661652e-01  4.55168874e-01  4.46276890e-01\n",
      "   6.02614079e-01 -3.13098518e-03  5.24323817e-04]\n",
      " [ 4.24913712e-01 -3.20312078e-01 -5.22365058e-02  3.34339481e-01\n",
      "  -7.43689883e-03  5.08853525e-04 -7.76067911e-01]\n",
      " [-2.44487304e-01 -5.67457559e-01 -1.13315588e-01 -2.79263154e-01\n",
      "  -7.80031747e-02 -7.21913853e-01 -1.20598098e-02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pca(X, num_components):\n",
    "    \"\"\"\n",
    "    Perform Principal Component Analysis on the input data.\n",
    "\n",
    "    Parameters:\n",
    "    - X: NumPy array, shape (n_samples, n_features)\n",
    "        Input data matrix.\n",
    "    - num_components: int\n",
    "        Number of principal components to retain.\n",
    "\n",
    "    Returns:\n",
    "    - X_pca: NumPy array, shape (n_samples, num_components)\n",
    "        Data matrix after PCA transformation.\n",
    "    - eigenvectors: NumPy array, shape (n_features, num_components)\n",
    "        Principal components (eigenvectors).\n",
    "    \"\"\"\n",
    "\n",
    "    # Standardize the data (subtract mean and divide by standard deviation)\n",
    "    X_standardized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    covariance_matrix = np.cov(X_standardized, rowvar=False)\n",
    "\n",
    "    # Compute the eigenvectors and eigenvalues of the covariance matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "    # Sort eigenvectors in descending order of eigenvalues\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "\n",
    "    # Select the top 'num_components' eigenvectors\n",
    "    selected_eigenvectors = eigenvectors[:, :num_components]\n",
    "\n",
    "    # Project the data onto the new subspace defined by the selected eigenvectors\n",
    "    X_pca = np.dot(X_standardized, selected_eigenvectors)\n",
    "\n",
    "    return X_pca, selected_eigenvectors\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'data' is your dataset as a NumPy array (n_samples, n_features)\n",
    "# and you want to reduce it to 2 principal components:\n",
    "num_components = 7 #num columns of dataset :)\n",
    "data_pca, principal_components = pca(data, num_components)\n",
    "#print(data_pca)\n",
    "print(principal_components)\n",
    "# 'data_pca' now contains the data transformed into the principal component space.\n",
    "# 'principal_components' contains the principal components (eigenvectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m YC \u001b[38;5;241m=\u001b[39m \u001b[43mXc\u001b[49m\u001b[38;5;241m.\u001b[39mdot(principal_components)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the dimension of the Y matrix \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m(*) using the loadings of the covariance matrix C\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m(*) It must be equal to the dimension of the original dataset\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(np\u001b[38;5;241m.\u001b[39mshape(YC)))\n\u001b[0;32m      5\u001b[0m total_variation_covariance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(principal_components)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xc' is not defined"
     ]
    }
   ],
   "source": [
    "YC = Xc.dot(principal_components)\n",
    "print(\"This is the dimension of the Y matrix \\t {}\\n\\t(*) using the loadings of the covariance matrix C\\n\"\n",
    "      \"\\t(*) It must be equal to the dimension of the original dataset\\n\".format(np.shape(YC)))\n",
    "\n",
    "total_variation_covariance = np.sum(principal_components)\n",
    "explained_variance_covariance = np.asarray(\n",
    "    [100*(i/total_variation_covariance) for i in sorted(principal_components, reverse=True)])\n",
    "print(\"This is the explained variance of each feature (covariance):\\n\\t{}\"\n",
    "      .format(np.array_str(explained_variance_covariance, precision=2)))\n",
    "cumulative_covariance = np.cumsum(explained_variance_covariance)\n",
    "print(\"This is the cumulative variance (covariance):\\n\\t{}\"\n",
    "      .format(np.array_str(cumulative_covariance, precision=2)))\n",
    "\n",
    "fig1 = plt.figure(1, figsize=(10,6))\n",
    "plt.title(\"Scree plots of Covariance Matrix\")\n",
    "plt.bar(x=np.arange(np.shape(explained_variance_covariance)[0]), \n",
    "        height=explained_variance_covariance, \n",
    "        width=0.4, color=\"green\")\n",
    "plt.plot(np.arange(np.shape(explained_variance_covariance)[0]), \n",
    "         explained_variance_covariance, \n",
    "         linestyle=\"--\", marker=\"o\", markersize=15,\n",
    "         color=\"red\", label=\"explained variance (covariance)\")\n",
    "plt.savefig(\"images/screeplot_covariance_pizza.png\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.23092648e+01  2.19301430e+01]\n",
      " [ 6.25284351e+00  1.47215152e+01]\n",
      " [ 6.01844335e+00  1.41531568e+01]\n",
      " [ 6.29228507e+00  1.48069400e+01]\n",
      " [ 1.25030980e+00  2.94149551e+00]\n",
      " [ 2.51310165e-01  5.91299847e-01]\n",
      " [-1.98096179e+01 -4.66130277e+01]\n",
      " [ 1.34143953e-02  3.12946481e-02]]\n",
      "[[-0.0199403  -0.00581763]\n",
      " [ 0.06447855 -0.62837526]\n",
      " [ 0.37895107 -0.26960964]\n",
      " [ 0.4464674   0.2342153 ]\n",
      " [ 0.47181383 -0.11106505]\n",
      " [ 0.43555098  0.20152183]\n",
      " [-0.42475578  0.32041655]\n",
      " [ 0.24449424  0.5674075 ]]\n",
      "0.12789226829508005\n",
      "0.9729892544783628\n",
      "0.17555226564037452\n",
      "0.7704822676787337\n",
      "0.6124818058867489\n",
      "0.26259222386287184\n",
      "0.030332809415548834\n",
      "0.3558462743058536\n",
      "0.7309578469359238\n",
      "0.7993116088147381\n",
      "[[14.764566240619372], [14.764566240619372], [14.764566240619375], [14.764566240619379], [14.76456624061937], [14.764566240619361], [14.764566240619363], [14.764566240619372], [14.764566240619372], [14.764566240619375]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ppca(X, num_components,W_init,sigma=None, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Perform Probabilistic Principal Component Analysis on the input data.\n",
    "\n",
    "    Parameters:\n",
    "    - X: NumPy array, shape (n_samples, n_features)\n",
    "        Input data matrix.\n",
    "    - num_components: int\n",
    "        Number of latent dimensions (principal components).\n",
    "    - max_iter: int, optional\n",
    "        Maximum number of iterations for the EM algorithm.\n",
    "    - tol: float, optional\n",
    "        Convergence tolerance.\n",
    "\n",
    "    Returns:\n",
    "    - W: NumPy array, shape (n_features, num_components)\n",
    "        Projection matrix (principal components).\n",
    "    - Z: NumPy array, shape (n_samples, num_components)\n",
    "        Latent variables.\n",
    "    - sigma_squared: float\n",
    "        Noise variance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Center the data\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_centered = X - X_mean\n",
    "\n",
    "    # parameter initialisation\n",
    "    n_samples, n_features = X.shape\n",
    "    '''\n",
    "    if W_init.any() != None:\n",
    "        W=W_init\n",
    "    else:\n",
    "        W = np.random.randn(n_features, num_components)\n",
    "    '''\n",
    "    W=W_init\n",
    "\n",
    "    if sigma !=None:\n",
    "        sigma_squared=sigma\n",
    "    else:\n",
    "        sigma_squared=1\n",
    "    \n",
    "    \n",
    "    Z = np.dot(X_centered, W)\n",
    "    sigma_squared = 1.0\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        # E-step: Compute the posterior distribution over latent variables\n",
    "        M = np.dot(W.T, W) + sigma_squared * np.eye(num_components)\n",
    "        E_z = np.linalg.solve(M, np.dot(W.T, X_centered.T)).T\n",
    "        \n",
    "        # M-step: Update parameters\n",
    "        W = np.dot(X_centered.T, E_z) @ np.linalg.inv(np.dot(E_z.T, E_z))\n",
    "        sigma_squared = np.sum(np.square(X_centered - np.dot(E_z, W.T))) / (n_samples * n_features)\n",
    "\n",
    "        #print(E_z.shape)\n",
    "        #print(np.dot(X_centered, W))\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(E_z - np.dot(X_centered, W)) < tol:\n",
    "            print(iteration)\n",
    "            break\n",
    "\n",
    "    return W, E_z, sigma_squared\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'data' is your dataset as a NumPy array (n_samples, n_features)\n",
    "# and you want to reduce it to 2 latent dimensions:\n",
    "num_components = 2\n",
    "W_init = np.random.randn(data.shape[1], num_components)\n",
    "projection_matrix, latent_variables, sigma_squared = ppca(data, num_components,W_init)\n",
    "print(projection_matrix)\n",
    "print(principal_components)\n",
    "# 'projection_matrix' contains the projection matrix (principal components).\n",
    "# 'latent_variables' contains the latent variables.\n",
    "# 'noise_variance' contains the estimated noise variance.\n",
    "\n",
    "num_trials=10\n",
    "sigma_lst=[]\n",
    "W_lst=[]\n",
    "for _ in range(num_trials):\n",
    "    # Random initialization of W and sigma\n",
    "    W_init = np.random.randn(data.shape[1], num_components)\n",
    "    \n",
    "    sigma_init = np.random.rand()\n",
    "\n",
    "    print(sigma_init)\n",
    "    # PPCA with random initialization\n",
    "    W, E_z, sigma_squared = ppca(data, num_components, W_init, sigma_init,10000,1e-20)\n",
    "    sigma_lst.append([sigma_squared])\n",
    "    W_lst.append(W)\n",
    "\n",
    "print(sigma_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages=4-7\n",
    "# change additional parameters of ppca!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
